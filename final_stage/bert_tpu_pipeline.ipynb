{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "bert_tpu_pipeline",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbh-uzFuKjfh"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqYet62tRyP5"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnJpOq_QMRI8"
      },
      "source": [
        "!pip install transformers sentencepiece \n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3mgJE8Xje7o"
      },
      "source": [
        "# !pip uninstall torch\n",
        "# !pip inst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch_xla\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch_xla.core.xla_model as xm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig, get_linear_schedule_with_warmup, AutoModelForQuestionAnswering, AutoModel, AutoConfig, AutoTokenizer\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.debug.metrics as met\n",
        "import os\n",
        "os.environ['XLA_USE_BF16']=\"1\"\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6w1hh1Ej31",
        "outputId": "40dcb887-7a64-473d-b918-e45b11074613"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YpnS89Xt7Es",
        "outputId": "3b916e4b-d9de-437b-a643-1e6ea15e29c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYkWWqeFt8Rh",
        "outputId": "409e7904-c5c2-4916-b9ac-3c1479c6b355"
      },
      "source": [
        "%cd drive/MyDrive/nti/rucos/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/nti/rucos/'\n",
            "/content/drive/MyDrive/nti/rucos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E21atncSJVGA",
        "outputId": "f1e3d7b4-de88-4f9a-97a3-6991a73647e7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch-xla-env-setup.py\n",
            "sbersquad_train.json\n",
            "test.jsonl\n",
            "torch-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torch-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "train.jsonl\n",
            "val.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FkV7uSit4J6",
        "outputId": "fd23d9c7-e99c-4c2e-cc99-f3abe80ce567"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('train.jsonl', 'r', encoding='utf-8') as data:\n",
        "  dat = list(data)\n",
        "ex = json.loads(dat[6])\n",
        "# for el in ex['passage']:\n",
        "#   print(el)\n",
        "\n",
        "print(f\"text : {ex['passage']['text']}\")\n",
        "print(f\"entities : {ex['passage']['entities']}\")\n",
        "print(f\"qas : {ex['qas']}\")\n",
        "print(ex['qas'][0]['query'])\n",
        "print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text : Требование и.о. чеченского премьера прекратить деятельность Датского совета помощи беженцам расценивается немецкими журналистами как политическая и пиар-акция, не имеющая ничего общего с конфликтом вокруг карикатур. Через несколько дней после начала массовых демонстраций протеста в мусульманских странах, вызванных публикацией в ряде западноевропейских газет карикатур на пророка Мухаммеда, и.о. премьер-министра Чечни Рамзан Кадыров фактически потребовал объявить работающих в республике сотрудников Датский совет по беженцам персонами \"нон грата\". Антидатская демонстрация в Индонезии\n",
            "Указанная им причина – Чечня присоединяется к \"бойкоту всего датского\", который объявлен в ряде исламских стран. Однако если, например, в Саудовской Аравии речь идет прежде всего о товарах датского производства, то руководство Чечне обратило свои санкции против граждан Дании, оказывающих гуманитарную помощь. Концентрация власти или вводная сверху\n",
            "Размышляя о заявлении Кадырова, которое он сделал 6 февраля, немецкая газета Die Welt в статье под заголовком \"Датчане, вон из Чечни!\" размышляет об истинных мотивах действий и.о. главы правительства этого российского региона. После того, как официально назначенный Кремлем премьер-министр республики Сергей Абрамов в декабре прошлого года попал в автомобильную катастрофу, пишет Die Welt, Рамзан Кадыров сосредоточил вокруг себя еще больше власти.\n",
            "@highlight\n",
            "Германия прислушивается к сигналам, но стоит на своем\n",
            "@highlight\n",
            "Справка: хроника \"карикатурного конфликта\"\n",
            "@highlight\n",
            "В Швеции задержаны двое граждан РФ в связи с нападением на чеченского блогера\n",
            "entities : [{'start': 60, 'end': 91}, {'start': 414, 'end': 419}, {'start': 420, 'end': 434}, {'start': 502, 'end': 527}, {'start': 578, 'end': 587}, {'start': 611, 'end': 616}, {'start': 726, 'end': 743}, {'start': 815, 'end': 820}, {'start': 858, 'end': 863}, {'start': 959, 'end': 967}, {'start': 1014, 'end': 1022}, {'start': 1064, 'end': 1069}, {'start': 1203, 'end': 1210}, {'start': 1238, 'end': 1252}, {'start': 1317, 'end': 1325}, {'start': 1327, 'end': 1341}, {'start': 1397, 'end': 1405}, {'start': 1518, 'end': 1524}, {'start': 1548, 'end': 1550}]\n",
            "qas : [{'query': '@placeholder, по мнению немецких журналистов, всего лишь использовал взрыв возмущения, вызванного в мусульманском мире публикацией карикатур на пророка Мухаммеда, для того, чтобы укрепить собственные позиции в республике.', 'answers': [{'start': 420, 'end': 434, 'text': 'Рамзан Кадыров'}, {'start': 959, 'end': 967, 'text': 'Кадырова'}, {'start': 1327, 'end': 1341, 'text': 'Рамзан Кадыров'}], 'idx': 6}]\n",
            "@placeholder, по мнению немецких журналистов, всего лишь использовал взрыв возмущения, вызванного в мусульманском мире публикацией карикатур на пророка Мухаммеда, для того, чтобы укрепить собственные позиции в республике.\n",
            "{'idx': 6, 'passage': {'text': 'Требование и.о. чеченского премьера прекратить деятельность Датского совета помощи беженцам расценивается немецкими журналистами как политическая и пиар-акция, не имеющая ничего общего с конфликтом вокруг карикатур. Через несколько дней после начала массовых демонстраций протеста в мусульманских странах, вызванных публикацией в ряде западноевропейских газет карикатур на пророка Мухаммеда, и.о. премьер-министра Чечни Рамзан Кадыров фактически потребовал объявить работающих в республике сотрудников Датский совет по беженцам персонами \"нон грата\". Антидатская демонстрация в Индонезии\\nУказанная им причина – Чечня присоединяется к \"бойкоту всего датского\", который объявлен в ряде исламских стран. Однако если, например, в Саудовской Аравии речь идет прежде всего о товарах датского производства, то руководство Чечне обратило свои санкции против граждан Дании, оказывающих гуманитарную помощь. Концентрация власти или вводная сверху\\nРазмышляя о заявлении Кадырова, которое он сделал 6 февраля, немецкая газета Die Welt в статье под заголовком \"Датчане, вон из Чечни!\" размышляет об истинных мотивах действий и.о. главы правительства этого российского региона. После того, как официально назначенный Кремлем премьер-министр республики Сергей Абрамов в декабре прошлого года попал в автомобильную катастрофу, пишет Die Welt, Рамзан Кадыров сосредоточил вокруг себя еще больше власти.\\n@highlight\\nГермания прислушивается к сигналам, но стоит на своем\\n@highlight\\nСправка: хроника \"карикатурного конфликта\"\\n@highlight\\nВ Швеции задержаны двое граждан РФ в связи с нападением на чеченского блогера', 'entities': [{'start': 60, 'end': 91}, {'start': 414, 'end': 419}, {'start': 420, 'end': 434}, {'start': 502, 'end': 527}, {'start': 578, 'end': 587}, {'start': 611, 'end': 616}, {'start': 726, 'end': 743}, {'start': 815, 'end': 820}, {'start': 858, 'end': 863}, {'start': 959, 'end': 967}, {'start': 1014, 'end': 1022}, {'start': 1064, 'end': 1069}, {'start': 1203, 'end': 1210}, {'start': 1238, 'end': 1252}, {'start': 1317, 'end': 1325}, {'start': 1327, 'end': 1341}, {'start': 1397, 'end': 1405}, {'start': 1518, 'end': 1524}, {'start': 1548, 'end': 1550}]}, 'qas': [{'query': '@placeholder, по мнению немецких журналистов, всего лишь использовал взрыв возмущения, вызванного в мусульманском мире публикацией карикатур на пророка Мухаммеда, для того, чтобы укрепить собственные позиции в республике.', 'answers': [{'start': 420, 'end': 434, 'text': 'Рамзан Кадыров'}, {'start': 959, 'end': 967, 'text': 'Кадырова'}, {'start': 1327, 'end': 1341, 'text': 'Рамзан Кадыров'}], 'idx': 6}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUYoew-zJQWg",
        "outputId": "3f6f2df7-44eb-4eb5-a839-a486c2a7848c"
      },
      "source": [
        "print"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function print>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urBZ6ADutZjH"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/xlm-roberta-large-squad2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oV1dO2fk1eJ"
      },
      "source": [
        "train_text = []\n",
        "train_query = []\n",
        "answers = []\n",
        "for example in dat[:20000]:\n",
        "  ex = json.loads(example)\n",
        "  text = ex['passage']['text']\n",
        "  query = ex['qas'][0]['query']\n",
        "  # length.append(len(tokenizer.tokenize(text)))\n",
        "  for i in ex['qas'][0]['answers']:\n",
        "    answers.append([i['start'], i['end']])\n",
        "    train_text.append(text[:256])\n",
        "    train_query.append(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUVcoJme7zP3"
      },
      "source": [
        "with open('val.jsonl', 'r', encoding='utf-8') as data:\n",
        "  dat = list(data)\n",
        "\n",
        "val_text = []\n",
        "val_query = []\n",
        "val_answers = []\n",
        "for example in dat[:20000]:\n",
        "  ex = json.loads(example)\n",
        "  text = ex['passage']['text']\n",
        "  query = ex['qas'][0]['query']\n",
        "  # length.append(len(tokenizer.tokenize(text)))\n",
        "  for i in ex['qas'][0]['answers']:\n",
        "    val_answers.append([i['start'], i['end']])\n",
        "    val_text.append(text[:256])\n",
        "    val_query.append(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtkYE9OFdY4C"
      },
      "source": [
        "pad_on_right = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj-UhySeXTgK"
      },
      "source": [
        "def prepare_train_features(questions, texts, labels):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "  tokenized_examples = tokenizer(questions, texts, \n",
        "                              max_length=450,\n",
        "                              padding='max_length',\n",
        "                              stride=128,\n",
        "                              truncation=True,\n",
        "                              return_overflowing_tokens=True,\n",
        "                              return_offsets_mapping=True\n",
        "                              )\n",
        "\n",
        "\n",
        "  sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "  offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "  tokenized_examples[\"start_positions\"] = []\n",
        "  tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "  for i, offsets in enumerate(offset_mapping):\n",
        "      # We will label impossible answers with the index of the CLS token.\n",
        "      input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "      cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "      # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "      sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "      # One example can give several spans, this is the index of the example containing this span of text.\n",
        "      sample_index = sample_mapping[i]\n",
        "      answers = labels[sample_index]\n",
        "      # If no answers are given, set the cls_index as answer.\n",
        "      if len(answers) == 0:\n",
        "          tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "          tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "      else:\n",
        "          # Start/end character index of the answer in the text.\n",
        "          start_char = answers[0]\n",
        "          end_char = answers[1]\n",
        "\n",
        "          # Start token index of the current span in the text.\n",
        "          token_start_index = 0\n",
        "          while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "              token_start_index += 1\n",
        "\n",
        "          # End token index of the current span in the text.\n",
        "          token_end_index = len(input_ids) - 1\n",
        "          while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "              token_end_index -= 1\n",
        "\n",
        "          # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "          if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "              tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "              tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "          else:\n",
        "              # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "              # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "              while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                  token_start_index += 1\n",
        "              tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "              while offsets[token_end_index][1] >= end_char:\n",
        "                  token_end_index -= 1\n",
        "              tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "  return tokenized_examples\n",
        "train_data = prepare_train_features(questions=train_query, texts=train_text, labels=answers)\n",
        "validation_data = prepare_train_features(questions=val_text,texts=val_text, labels=val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWzN9kEbBGZB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9urjuJKsyOmd"
      },
      "source": [
        "FLAGS = {}\n",
        "# FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "# FLAGS['batch_size'] = 4\n",
        "FLAGS['num_workers'] = 8\n",
        "FLAGS['learning_rate'] = 2e-5\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 8\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 1\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "0f3e2dcf-cad8-4ca1-d522-81df1b1ab969"
      },
      "source": [
        "print(os.environ.get(('TPU_NAME'), None))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grpc://10.99.144.82:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Anba80AwUGq",
        "outputId": "6ca51e0b-266a-4146-f0a5-105cae30f6c8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch-xla-env-setup.py\n",
            "sbersquad_train.json\n",
            "test.jsonl\n",
            "torch-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torch-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torchvision-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-1.6-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\n",
            "torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\n",
            "train.jsonl\n",
            "val.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGkt6Ne51Iz3"
      },
      "source": [
        "# Sber(old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjvhNQbGEJC0"
      },
      "source": [
        "# import transformers\n",
        "# from transformers import XLMRobertaTokenizerFast\n",
        "# tokenizer = XLMRobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "# assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "# tokenizer.padding_side = \"right\"\n",
        "# assert len(question_train) == len(text_train)\n",
        "# pad_on_right = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1v-1SW-DMAA"
      },
      "source": [
        "#@title prepare train feautures\n",
        "# def prepare_train_features(questions, texts):\n",
        "#     # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "#     # in one example possible giving several features when a context is long, each of those features having a\n",
        "#     # context that overlaps a bit the context of the previous feature.\n",
        "#   tokenized_examples = tokenizer(questions, texts, \n",
        "#                               max_length=512,\n",
        "#                               padding='max_length',\n",
        "#                               stride=128,\n",
        "#                               truncation=True,\n",
        "#                               return_overflowing_tokens=True,\n",
        "#                               return_offsets_mapping=True\n",
        "#                               )\n",
        "\n",
        "\n",
        "#   sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "#   offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "#   tokenized_examples[\"start_positions\"] = []\n",
        "#   tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "#   for i, offsets in enumerate(offset_mapping):\n",
        "#       # We will label impossible answers with the index of the CLS token.\n",
        "#       input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "#       cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "#       # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "#       sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "#       # One example can give several spans, this is the index of the example containing this span of text.\n",
        "#       sample_index = sample_mapping[i]\n",
        "#       answers = labels[sample_index]\n",
        "#       # If no answers are given, set the cls_index as answer.\n",
        "#       if len(answers) == 0:\n",
        "#           tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "#           tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "#       else:\n",
        "#           # Start/end character index of the answer in the text.\n",
        "#           start_char = answers[0]\n",
        "#           end_char = answers[1]\n",
        "\n",
        "#           # Start token index of the current span in the text.\n",
        "#           token_start_index = 0\n",
        "#           while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "#               token_start_index += 1\n",
        "\n",
        "#           # End token index of the current span in the text.\n",
        "#           token_end_index = len(input_ids) - 1\n",
        "#           while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "#               token_end_index -= 1\n",
        "\n",
        "#           # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "#           if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "#               tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "#               tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "#           else:\n",
        "#               # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "#               # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "#               while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "#                   token_start_index += 1\n",
        "#               tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "#               while offsets[token_end_index][1] >= end_char:\n",
        "#                   token_end_index -= 1\n",
        "#               tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "#   return tokenized_examples\n",
        "# train_data = prepare_train_features(question_train[:10000], text_train[:10000])\n",
        "# validation_data = prepare_train_features(question_train[11000:12001],text_train[11000:12001])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZNL4Cwd7SG_"
      },
      "source": [
        "# ну раздел и раздел\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_WMzbWzkhFB"
      },
      "source": [
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from torch import nn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "NUM_EPOCHS = 8\n",
        "BATCH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "source": [
        "def preprocessing(train_inputs, validation_data,\n",
        "                  batch_size):\n",
        "    # train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    #     input_ids, train_gt_orig, \n",
        "    #     random_state=42,\n",
        "    #     test_size=test_SIZE\n",
        "    # )\n",
        "\n",
        "    train_in = torch.tensor(train_inputs['input_ids'], dtype=torch.long)\n",
        "    train_mask = torch.tensor(train_inputs['attention_mask'])\n",
        "    # train_labels = torch.tensor(train_inputs['start_score'], dtype=torch.long)\n",
        "    start_score = torch.tensor(train_inputs['start_positions'], dtype=torch.long)\n",
        "    end_score = torch.tensor(train_inputs['end_positions'], dtype=torch.long)\n",
        "    training_data = TensorDataset(train_in, train_mask,start_score, end_score)\n",
        "\n",
        "\n",
        "\n",
        "    validation_inputs = torch.tensor(validation_data['input_ids'], dtype=torch.long)\n",
        "    validation_mask = torch.tensor(validation_data['attention_mask'], dtype=torch.long)\n",
        "    validation_start = torch.tensor(validation_data['start_positions'], dtype=torch.long)\n",
        "    validation_end = torch.tensor(validation_data['end_positions'], dtype=torch.long)\n",
        "    # valid_labels = torch.tensor(valid_labels, dtype=torch.long)\n",
        "    valid_data = TensorDataset(validation_inputs, validation_start,validation_start, validation_end)\n",
        "\n",
        "    \n",
        "\n",
        "    train_sampler =  torch.utils.data.distributed.DistributedSampler(   #SequentialSampler(train_data)\n",
        "      training_data,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=False)\n",
        "    train_dataloader = DataLoader(\n",
        "        training_data,\n",
        "        sampler=train_sampler,\n",
        "        batch_size=batch_size,\n",
        "        num_workers = 0\n",
        "    )\n",
        "    \n",
        "    \n",
        "    val_sampler = torch.utils.data.distributed.DistributedSampler(     #SequentialSampler(valid_data)\n",
        "      valid_data,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=False) \n",
        "    validation_dataloader = DataLoader(\n",
        "        valid_data,\n",
        "        sampler=val_sampler,\n",
        "        batch_size=batch_size,\n",
        "        num_workers = 0\n",
        "    )\n",
        "    \n",
        "    \n",
        "    return train_dataloader, validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tafd0JbUExb7"
      },
      "source": [
        "train_dataloader, validation_dataloader = preprocessing(train_inputs=train_data,\n",
        "                                                          validation_data=validation_data,\n",
        "                                                      batch_size=BATCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvHMSXpq5jWT"
      },
      "source": [
        "# class DBQA(torch.nn.Module):\n",
        "#     def __init__(self, m):\n",
        "#         super(DBQA, self).__init__()    \n",
        "#         self.model= m\n",
        "#         self.d1 = torch.nn.Dropout(0.3)\n",
        "#         self.l1 = torch.nn.Linear(768, 2)\n",
        "#         self.model.apply_state()\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         _, x  = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         x = self.d1(x)\n",
        "#         x = self.l1(x)\n",
        "#         return x  \n",
        "# m = AutoModel.from_pretrained(\"google/mt5-large\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJku2ZGvb5SQ"
      },
      "source": [
        "train_loss_set = []\n",
        "import time\n",
        "def model_training():\n",
        "  # global train_loss_set\n",
        "\n",
        "  # del inputs_train, inputs_valid\n",
        "  torch.manual_seed(42)\n",
        "  device = xm.xla_device()\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/xlm-roberta-large-squad2\")\n",
        "  clear_output()\n",
        "  # devices = (\n",
        "  #   xm.get_xla_supported_devices(\n",
        "  #       max_devices=FLAGS['num_cores']) if FLAGS['num_cores'] != 0 else [])\n",
        "  \n",
        "  # model = xmp.MpModelWrapper(m)\n",
        "  model = model.to(device)\n",
        "  \n",
        "  \n",
        "  lr = 2e-5 * 8\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=len(train_dataloader), \n",
        "                                            num_training_steps=len(train_dataloader)*NUM_EPOCHS)\n",
        "  \n",
        "  \n",
        "  def train(loader):                     # train_loss_set = []\n",
        "      model.train()                                     # train_loss = 0\n",
        "      tracker = xm.RateTracker()\n",
        "      for step, batch in enumerate(loader):\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          input_ids, mask, start, end = batch\n",
        "          optimizer.zero_grad()\n",
        "          loss = model(input_ids,mask, start, end)\n",
        "          # train_loss_set.append(loss[0].item())  \n",
        "          loss[0].backward()\n",
        "          xm.optimizer_step(optimizer)\n",
        "          scheduler.step()\n",
        "          # clear_output()\n",
        "          tracker.add(BATCH)\n",
        "          # if step % 100 == 0:\n",
        "          print('[xla:{}]({}/{}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "                xm.get_ordinal(), step,len(train_dataloader), loss[0].item(), tracker.rate(),\n",
        "                tracker.global_rate(), time.asctime()[10:-5]), flush=True)\n",
        "\n",
        "      \n",
        "    \n",
        "\n",
        "\n",
        "  def evaluation(dataloader):\n",
        "      global start_logits, end_logits\n",
        "      model.eval()\n",
        "      valid_labels, valid_preds = [], []\n",
        "      for batch in dataloader:   \n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          \n",
        "          input_ids, mask, start, end= batch\n",
        "          with torch.no_grad():\n",
        "              logits =  model(input_ids,mask, start, end)\n",
        "          # logits = logits[0].\n",
        "          preds = logits[0].to('cpu').numpy()\n",
        "          batch_preds = np.argmax(preds, axis=1)\n",
        "          batch_labels = np.stack(labels, axis=0) \n",
        "          valid_preds.append(batch_preds) \n",
        "          batch_acc = accuracy_score(valid_preds, valid_labels)\n",
        "          batch_f1 = f1_score(valid_preds, valid_labels )\n",
        "          plt.sublpot(batch_acc)\n",
        "          plt.ylabel('accuracy')\n",
        "          plt.show()\n",
        "          plt.subplot(batch_f1)\n",
        "          plt.ylabel('f1')\n",
        "          plt.show()\n",
        "      return valid_preds\n",
        "    \n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "      para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
        "      train(para_loader.per_device_loader(device))\n",
        "      xm.master_print(\"---------------------------------------------- Finished training epoch {} -------------------------------\".format(epoch))\n",
        "\n",
        "      para_loader = pl.ParallelLoader(validation_dataloader, [device])\n",
        "      pred, target  = evaluation(para_loader.per_device_loader(device))\n",
        "  \n",
        "  \n",
        "  \n",
        "  return valid_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgqFvykcbYFw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJXpKqfLxGQh",
        "outputId": "03b06753-5faf-4287-e3ec-916d1d9f39aa"
      },
      "source": [
        "!cat /proc/cpuinfo | grep processor | wc -l\n",
        "!free -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G         10G        207M        2.2M        2.4G        6.0G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "YDmY4OPsNT4A",
        "outputId": "5d65c336-a8bc-4ca9-8658-d25810e21561"
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  preds = model_training()\n",
        "  FLAGS = flags\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ProcessExitedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-1fd78d2e56f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0msignal_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 )\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with signal SIGKILL"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq_rIXDsHDtU"
      },
      "source": [
        "def postprocess_qa_predictions(examples, features, start, end, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = start_logits, end_logits\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3Gy1fkh1WB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}